---
title: "Lab 8a"
author: "GAO Zheng from last year"
date: "October 27, 2017"
output:
  html_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Now you can download the RMarkdown file by clicking the button in the rupper right corner of this html page.

## Law of Large Numbers

Write a function which calculates the empirical distribution function of $X_1,\cdots,X_m\stackrel{\text{iid}}{\sim}N(0,1)$. The empirical distribution function is defined as
$$
\Phi_m(x) = \frac{1}{m}\sum_{i=1}^mI(X_i\leq x)
$$
where $I(X\leq x)$ is an indicator function, taking value 1 if $X\leq x$ and 0 otherwise. Plot this function for $x\in[-4,4]$. Let $m=5, 50, 500$ respectively and plot all three empirical CDF's on the same figure, with the true standard Normal cdf superimposed for comparison.

```{r}
Phi.m = function(m,xseq){
	# xseq can be either a scalar or a vector
	X = rnorm(m)
	Phi.m.x = sapply(xseq,function(x){mean(X<x)})
	return(Phi.m.x)
}
m = c(5,50,500)
xseq = seq(-4,4,by=0.01)
plot(0,xlim=c(-4,4),ylim=c(0,1),xlab="x",ylab="Phi.m(x)",
     main="Empirical CDF", type='n')
for (i in 1:length(m)){
	par(new=T)
	plot(xseq,Phi.m(m[i],xseq),col=i,axes=F,type="l",xlim=c(-4,4),
	ylim=c(0,1),xlab="",ylab="")
}
lines(xseq,pnorm(xseq,mean=0,sd=1),col=4,lty=3,lwd=2)
legend("topleft",legend=c(paste(m,'points'),'Normal cdf'),
       col=1:4,lty=c(1,1,1,3))
```

**Remark**: here for almost all $x$, the value $\Phi_m(x)$ is approaching the Normal cdf. Why does this happen? Recall the Law of Large Numbers: for any fixed $x$, $\Phi_m(x)$ is the mean of $m$ independent identically distributed random variables $I(X_i\leq x)$, therefore it will converge to the expectation $E(I(X_1\leq x))=P(X_{1}\leq x)$, which is the standard Normal cdf.

In other words, the empirical cdf converges point-wise to the true cdf

## Generating random variables with inversion method and more

Random variables from a particular distribution can be generated through uniform random variables by inverting their cumulative distribution function (cdf). That is, if you create i.i.d. samples $U_1, \cdots , U_n$ from $\textrm{Uniform}(0, 1)$ distribution and compute $F^{-1}(U_1), \cdots , F^{-1}(U_n)$ (assuming it is well-defined), then what you get is samples from a distribution with cdf $F$ . Using this property, we'll generate exponential random variables and gamma random variables.

*(a)*

The Exponential$(\lambda)$ distribution has cdf

$$
F(x) = 1 - e^{-\lambda x}, \lambda > 0
$$
Using `runif` function, generate 100 samples from `Exponential(3)` distribution using the inversion method. Graph the density histogram for the sample with the true density superimposed for comparison.

```{r}
n = 100
lambda = 3
x = -(1/lambda)*log(runif(n))
hist(x, prob = TRUE, ylim = c(0,3))
y = seq(0,10,length = 1000)
lines(y,dexp(y,3))
```


*(b)* 

$\textrm{Gamma}(k, \theta)~,~k>0,~\theta>0$ distribution has density:

$$f(x) = \frac{\theta^k}{(k-1)!}x^{k-1}e^{-\theta x}$$
where $k~and~\theta$ are shape and rate parameters. 

The exponential distribution with parameter $\lambda$ is a special case of of Gamma distribution.

**Q:** What are $k$ and $\theta$ for an Exponential($\lambda$)in the Gamma distribution?

Gamma distribution also has the property that if $X \sim Gamma(a, \theta)$ and $Y \sim Gamma(b, \theta)$, and $X$ and $Y$ are independent, then
$$X+Y \sim Gamma(a+b,\theta).$$ Using this property and the way to generate exponential distributions, create 10 samples from Gamma(5, 3) distribution.

```{r}
n = 10
k = 5
lambda = 3
x = matrix(-(1/lambda)*log(runif(n*k)), ncol=k)
(g = apply(x, 1, sum))
```

**Remark**: the density of exponential distribution is $\lambda e^{-\lambda x}$, which you can derive by taking the derivative of the cdf. Comparing with Gamma density, we see here $\theta=\lambda$ and $k=1$. We simulated the Gamma(5,3) as the sum of 5 independent exponential variables with $\lambda=3$.

## Inversion method for discrete random variables

Write a function to generate $n$ random numbers from binomial distribution with $m$ trials and $p$ using inversion sampling. The mass function of binomial distribution is
$$P(X=k)= {m \choose k} p^k (1-p)^{m-k}$$
where $k = 0,1,\ldots,m$. We set $n = 1000$, $p = 0.4$. Plot the histogram with $m = 10$.

```{r}
## Function to implement the inversion sampling
## for binomial distribution
binomial <- function(n, n_max, prob)
{
  ## Generate n uninform numbers
  z <- runif(n)
  ## Obtain P(X=0), P(X=1), ..., P(X=num_trails)
  k = seq(0, n_max)
  p <- choose(n_max, k)* prob^k * (1-prob)^(n_max-k)
  ## Intialize the binomial numbers
  x <- rep(0, n)
  for (i in seq(1, n))
  {
    s <- 0  ## Initialize the sum
    t <- -1  ## Initialize
    ## While loop
    ## Finding the smallest k such that p[1]+...p[t] > z[i]
    while (s < z[i])
    {
      t <- t + 1
      s <- s + p[t+1]
    }
    x[i] <- t
  }
  
  return(x)
}

## Call binomial generator
x <- binomial(n = 1000, n_max = 10, prob = 0.4)
hist(x, xlim = c(0, 10))
```

c.f. the R base binomial random vairable generator

```{r, results='hide',eval=FALSE}
x <- rbinom(n = 1000, size = 10, prob = 0.4)
hist(x, xlim = c(0, 10))
```


